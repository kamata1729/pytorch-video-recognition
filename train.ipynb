{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "from datetime import datetime\n",
    "import socket\n",
    "import os\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from tensorboardX import SummaryWriter\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from dataloaders.dataset import VideoDataset\n",
    "from network import C3D_model, R2Plus1D_model, R3D_model\n",
    "\n",
    "from slack_notification import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use GPU if available else revert to CPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device being used:\", device)\n",
    "\n",
    "nEpochs = 100  # Number of epochs for training\n",
    "resume_epoch = 0  # Default is 0, change if want to resume\n",
    "useTest = True # See evolution of the test set when training\n",
    "nTestInterval = 20 # Run on test set every nTestInterval epochs\n",
    "snapshot = 50 # Store a model every snapshot epochs\n",
    "lr = 1e-3 # Learning rate\n",
    "\n",
    "dataset = 'ucf101' # Options: hmdb51 or ucf101\n",
    "\n",
    "if dataset == 'hmdb51':\n",
    "    num_classes=51\n",
    "elif dataset == 'ucf101':\n",
    "    num_classes = 101\n",
    "else:\n",
    "    print('We only implemented hmdb and ucf datasets.')\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir_root = os.path.join(os.path.dirname(os.path.abspath(__file__)))\n",
    "exp_name = os.path.dirname(os.path.abspath(__file__)).split('/')[-1]\n",
    "\n",
    "if resume_epoch != 0:\n",
    "    runs = sorted(glob.glob(os.path.join(save_dir_root, 'run', 'run_*')))\n",
    "    run_id = int(runs[-1].split('_')[-1]) if runs else 0\n",
    "else:\n",
    "    runs = sorted(glob.glob(os.path.join(save_dir_root, 'run', 'run_*')))\n",
    "    run_id = int(runs[-1].split('_')[-1]) + 1 if runs else 0\n",
    "\n",
    "save_dir = os.path.join(save_dir_root, 'run', 'run_' + str(run_id))\n",
    "modelName = 'R3D' # Options: C3D or R2Plus1D or R3D\n",
    "saveName = modelName + '-' + dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(dataset=dataset, save_dir=save_dir, num_classes=num_classes, lr=lr,\n",
    "                num_epochs=nEpochs, save_epoch=snapshot, useTest=useTest, test_interval=nTestInterval):\n",
    "    \"\"\"\n",
    "        Args:\n",
    "            num_classes (int): Number of classes in the data\n",
    "            num_epochs (int, optional): Number of epochs to train for.\n",
    "    \"\"\"\n",
    "\n",
    "    if modelName == 'C3D':\n",
    "        model = C3D_model.C3D(num_classes=num_classes, pretrained=False)\n",
    "        train_params = [{'params': C3D_model.get_1x_lr_params(model), 'lr': lr},\n",
    "                        {'params': C3D_model.get_10x_lr_params(model), 'lr': lr * 10}]\n",
    "    elif modelName == 'R2Plus1D':\n",
    "        model = R2Plus1D_model.R2Plus1DClassifier(num_classes=num_classes, layer_sizes=(2, 2, 2, 2))\n",
    "        train_params = [{'params': R2Plus1D_model.get_1x_lr_params(model), 'lr': lr},\n",
    "                        {'params': R2Plus1D_model.get_10x_lr_params(model), 'lr': lr * 10}]\n",
    "    elif modelName == 'R3D':\n",
    "        model = R3D_model.R3DClassifier(num_classes=num_classes, layer_sizes=(2, 2, 2, 2))\n",
    "        train_params = model.parameters()\n",
    "    else:\n",
    "        print('We only implemented C3D and R2Plus1D models.')\n",
    "        raise NotImplementedError\n",
    "    criterion = nn.CrossEntropyLoss()  # standard crossentropy loss for classification\n",
    "    optimizer = optim.SGD(train_params, lr=lr, momentum=0.9, weight_decay=5e-4)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10,\n",
    "                                          gamma=0.1)  # the scheduler divides the lr by 10 every 10 epochs\n",
    "\n",
    "    if resume_epoch == 0:\n",
    "        print(\"Training {} from scratch...\".format(modelName))\n",
    "    else:\n",
    "        checkpoint = torch.load(os.path.join(save_dir, 'models', saveName + '_epoch-' + str(resume_epoch - 1) + '.pth.tar'),\n",
    "                       map_location=lambda storage, loc: storage)   # Load all tensors onto the CPU\n",
    "        print(\"Initializing weights from: {}...\".format(\n",
    "            os.path.join(save_dir, 'models', saveName + '_epoch-' + str(resume_epoch - 1) + '.pth.tar')))\n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['opt_dict'])\n",
    "\n",
    "    print('Total params: %.2fM' % (sum(p.numel() for p in model.parameters()) / 1000000.0))\n",
    "    model.to(device)\n",
    "    criterion.to(device)\n",
    "\n",
    "    log_dir = os.path.join(save_dir, 'models', datetime.now().strftime('%b%d_%H-%M-%S') + '_' + socket.gethostname())\n",
    "    \n",
    "\n",
    "    print('Training model on {} dataset...'.format(dataset))\n",
    "    train_dataloader = DataLoader(VideoDataset(dataset=dataset, split='train',clip_len=16), batch_size=20, shuffle=True, num_workers=4)\n",
    "    val_dataloader   = DataLoader(VideoDataset(dataset=dataset, split='val',  clip_len=16), batch_size=20, num_workers=4)\n",
    "    test_dataloader  = DataLoader(VideoDataset(dataset=dataset, split='test', clip_len=16), batch_size=20, num_workers=4)\n",
    "\n",
    "    trainval_loaders = {'train': train_dataloader, 'val': val_dataloader}\n",
    "    trainval_sizes = {x: len(trainval_loaders[x].dataset) for x in ['train', 'val']}\n",
    "    test_size = len(test_dataloader.dataset)\n",
    "\n",
    "    for epoch in range(resume_epoch, num_epochs):\n",
    "        # each epoch has a training and validation step\n",
    "        for phase in ['train', 'val']:\n",
    "            start_time = timeit.default_timer()\n",
    "\n",
    "            # reset the running loss and corrects\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0.0\n",
    "\n",
    "            # set model to train() or eval() mode depending on whether it is trained\n",
    "            # or being validated. Primarily affects layers such as BatchNorm or Dropout.\n",
    "            if phase == 'train':\n",
    "                # scheduler.step() is to be called once every epoch during training\n",
    "                scheduler.step()\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            for inputs, labels in tqdm(trainval_loaders[phase]):\n",
    "                # move inputs and labels to the device the training is taking place on\n",
    "                inputs = Variable(inputs, requires_grad=True).to(device)\n",
    "                labels = Variable(labels).to(device)\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                if phase == 'train':\n",
    "                    outputs = model(inputs)\n",
    "                else:\n",
    "                    with torch.no_grad():\n",
    "                        outputs = model(inputs)\n",
    "\n",
    "                probs = nn.Softmax(dim=1)(outputs)\n",
    "                preds = torch.max(probs, 1)[1]\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                if phase == 'train':\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / trainval_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / trainval_sizes[phase]\n",
    "            writer = SummaryWriter(logdir=log_dir)\n",
    "            if phase == 'train':\n",
    "                writer.add_scalar('data/train_loss_epoch', epoch_loss, epoch)\n",
    "                writer.add_scalar('data/train_acc_epoch', epoch_acc, epoch)\n",
    "            else:\n",
    "                writer.add_scalar('data/val_loss_epoch', epoch_loss, epoch)\n",
    "                writer.add_scalar('data/val_acc_epoch', epoch_acc, epoch)\n",
    "            writer.close()\n",
    "            \n",
    "            print(\"[{}] Epoch: {}/{} Loss: {} Acc: {}\".format(phase, epoch+1, nEpochs, epoch_loss, epoch_acc))\n",
    "            send_notification(text=\"[{}] Epoch: {}/{} Loss: {} Acc: {}\".format(phase, epoch+1, nEpochs, epoch_loss, epoch_acc))\n",
    "            stop_time = timeit.default_timer()\n",
    "            print(\"Execution time: \" + str(stop_time - start_time) + \"\\n\")\n",
    "\n",
    "        if epoch % save_epoch == (save_epoch - 1):\n",
    "            torch.save({\n",
    "                'epoch': epoch + 1,\n",
    "                'state_dict': model.state_dict(),\n",
    "                'opt_dict': optimizer.state_dict(),\n",
    "            }, os.path.join(save_dir, 'models', saveName + '_epoch-' + str(epoch) + '.pth.tar'))\n",
    "            print(\"Save model at {}\\n\".format(os.path.join(save_dir, 'models', saveName + '_epoch-' + str(epoch) + '.pth.tar')))\n",
    "\n",
    "        if useTest and epoch % test_interval == (test_interval - 1):\n",
    "            model.eval()\n",
    "            start_time = timeit.default_timer()\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0.0\n",
    "\n",
    "            for inputs, labels in tqdm(test_dataloader):\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    outputs = model(inputs)\n",
    "                probs = nn.Softmax(dim=1)(outputs)\n",
    "                preds = torch.max(probs, 1)[1]\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / test_size\n",
    "            epoch_acc = running_corrects.double() / test_size\n",
    "\n",
    "            writer = SummaryWriter(logdir=log_dir)\n",
    "            writer.add_scalar('data/test_loss_epoch', epoch_loss, epoch)\n",
    "            writer.add_scalar('data/test_acc_epoch', epoch_acc, epoch)\n",
    "            writer.close()\n",
    "            \n",
    "            print(\"[test] Epoch: {}/{} Loss: {} Acc: {}\".format(epoch+1, nEpochs, epoch_loss, epoch_acc))\n",
    "            stop_time = timeit.default_timer()\n",
    "            print(\"Execution time: \" + str(stop_time - start_time) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
